{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmCeF24ZUaH065/GkV6NM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chakrapani2122/Learning-List/blob/main/Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Mastering Machine Learning – Year-Long Roadmap\n",
        "\n",
        "---\n",
        "\n",
        "## 📅 Week-by-Week Timeline\n",
        "\n",
        "### **Phase 1 – Foundations (Weeks 1–8)**\n",
        "\n",
        "**Goal**: Build strong math, probability, statistics, and optimization intuition.\n",
        "\n",
        "| Week | Topics                             | Learning Activities                                                                             |\n",
        "| ---- | ---------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
        "| 1    | **Intro to ML + Linear Algebra I** | Vectors, matrices, dot product, matrix multiplication. Coding with NumPy.                       |\n",
        "| 2    | **Linear Algebra II**              | Eigenvalues, eigenvectors, SVD, PCA math foundations.                                           |\n",
        "| 3    | **Probability Basics**             | Random variables, distributions, Bayes’ theorem. Simulations in Python.                         |\n",
        "| 4    | **Statistics**                     | Estimation, confidence intervals, hypothesis testing.                                           |\n",
        "| 5    | **Calculus I**                     | Derivatives, gradients, chain rule with ML examples.                                            |\n",
        "| 6    | **Calculus II + Optimization**     | Gradient descent, convexity, constrained optimization.                                          |\n",
        "| 7    | **ML Workflow Basics**             | Datasets, preprocessing, train-test split, cross-validation.                                    |\n",
        "| 8    | **Project 1**                      | Implement PCA & linear regression from scratch. Compare NumPy implementation with scikit-learn. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Phase 2 – Core ML Algorithms (Weeks 9–20)**\n",
        "\n",
        "**Goal**: Learn every major ML algorithm in **theory → math → code (sklearn, TF, Keras, PyTorch).**\n",
        "\n",
        "| Week | Algorithm                        | What You’ll Do                                                                                             |\n",
        "| ---- | -------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
        "| 9    | **Linear Regression**            | Derive OLS, gradient descent solution. Implement in all 4 libs.                                            |\n",
        "| 10   | **Logistic Regression**          | Sigmoid, cross-entropy loss. Multiclass softmax. Experiments.                                              |\n",
        "| 11   | **k-NN & Naive Bayes**           | Distance metrics, Bayes classification. Implement + analyze.                                               |\n",
        "| 12   | **Decision Trees**               | Entropy, Gini index, CART derivation. Implement in sklearn & PyTorch.                                      |\n",
        "| 13   | **Random Forests & Bagging**     | Bootstrap, feature importance. Compare to single trees.                                                    |\n",
        "| 14   | **SVMs**                         | Margins, kernels, dual formulation. Train with sklearn + TF.                                               |\n",
        "| 15   | **k-Means & Clustering**         | Objective function, Lloyd’s algorithm. Compare sklearn & PyTorch.                                          |\n",
        "| 16   | **PCA, t-SNE, UMAP**             | Dimensionality reduction + visualization.                                                                  |\n",
        "| 17   | **Gradient Boosting & XGBoost**  | Boosting theory, implementation. Compare sklearn vs XGBoost vs PyTorch Lightning.                          |\n",
        "| 18   | **Evaluation Metrics Deep Dive** | Confusion matrix, precision, recall, F1, AUC-ROC, PR curves.                                               |\n",
        "| 19   | **Hyperparameter Tuning**        | Grid search, random search, Bayesian optimization.                                                         |\n",
        "| 20   | **Project 2**                    | Tabular dataset classification (Titanic/Loan prediction). Build models in all libraries, compare, analyze. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Phase 3 – Deep Learning Foundations (Weeks 21–32)**\n",
        "\n",
        "**Goal**: Transition into deep learning and neural networks.\n",
        "\n",
        "| Week | Topics                        | Learning Activities                                                                               |\n",
        "| ---- | ----------------------------- | ------------------------------------------------------------------------------------------------- |\n",
        "| 21   | **Intro to Neural Networks**  | Perceptrons, activation functions, forward/backpropagation.                                       |\n",
        "| 22   | **Training Deep Nets**        | Gradient descent, momentum, Adam, learning rate schedules.                                        |\n",
        "| 23   | **Regularization**            | Dropout, L1/L2, batch norm. Experiments.                                                          |\n",
        "| 24   | **CNNs I**                    | Convolutions, pooling, architectures (LeNet, AlexNet).                                            |\n",
        "| 25   | **CNNs II**                   | ResNet, Inception, transfer learning. Compare TF, Keras, PyTorch.                                 |\n",
        "| 26   | **RNNs & LSTMs**              | Sequence modeling, vanishing gradients.                                                           |\n",
        "| 27   | **GRUs & Seq2Seq**            | Applications in NLP. Implement in TF & PyTorch.                                                   |\n",
        "| 28   | **Transformers I**            | Self-attention, encoder-decoder. BERT, GPT basics.                                                |\n",
        "| 29   | **Transformers II**           | Fine-tuning Hugging Face models.                                                                  |\n",
        "| 30   | **Reinforcement Learning I**  | MDPs, Q-learning, value iteration.                                                                |\n",
        "| 31   | **Reinforcement Learning II** | Deep Q-Networks (DQN). Experiments in TF & PyTorch.                                               |\n",
        "| 32   | **Project 3**                 | Image classification (CIFAR-10) and NLP text classification. Train with CNNs, RNNs, Transformers. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Phase 4 – Advanced Topics & Research Readiness (Weeks 33–44)**\n",
        "\n",
        "**Goal**: Advanced methods, scalability, and deployment.\n",
        "\n",
        "| Week | Topics                         | Learning Activities                              |\n",
        "| ---- | ------------------------------ | ------------------------------------------------ |\n",
        "| 33   | **Unsupervised Deep Learning** | Autoencoders, variational autoencoders.          |\n",
        "| 34   | **Generative Models**          | GANs – derivation & training challenges.         |\n",
        "| 35   | **Graph Neural Networks**      | Basics of graph embeddings, GCNs.                |\n",
        "| 36   | **Advanced Optimization**      | Second-order methods, adaptive optimizers.       |\n",
        "| 37   | **Explainability**             | SHAP, LIME, feature attribution.                 |\n",
        "| 38   | **Model Deployment I**         | Saving/loading models, ONNX, TensorFlow Serving. |\n",
        "| 39   | **Model Deployment II**        | Flask/FastAPI + Docker for serving ML models.    |\n",
        "| 40   | **Experiment Tracking**        | MLflow, Weights & Biases.                        |\n",
        "| 41   | **Scaling ML**                 | Distributed training (Horovod, PyTorch DDP).     |\n",
        "| 42   | **Ethics & Fairness**          | Bias, fairness metrics, privacy in ML.           |\n",
        "| 43   | **AutoML**                     | Auto-sklearn, TPOT, AutoKeras.                   |\n",
        "| 44   | **Project 4**                  | End-to-end deployment of ML model (API + UI).    |\n",
        "\n",
        "---\n",
        "\n",
        "### **Phase 5 – Capstone (Weeks 45–52)**\n",
        "\n",
        "**Goal**: Mastery through **real-world projects**.\n",
        "\n",
        "| Week  | Project                            | Description                                               |\n",
        "| ----- | ---------------------------------- | --------------------------------------------------------- |\n",
        "| 45–46 | **Regression Project**             | House price prediction (tabular data). Deploy model.      |\n",
        "| 47–48 | **Classification Project**         | Fraud detection / medical diagnosis with imbalanced data. |\n",
        "| 49–50 | **NLP Project**                    | Sentiment analysis with transformers. Deploy API.         |\n",
        "| 51    | **Computer Vision Project**        | Object detection (YOLO/Faster-RCNN).                      |\n",
        "| 52    | **Reinforcement Learning Project** | Train RL agent on OpenAI Gym. Present as research paper.  |\n",
        "\n",
        "---\n",
        "\n",
        "## 📑 Syllabus Table\n",
        "\n",
        "| Module           | Topics                                                          | Resources                                                         |\n",
        "| ---------------- | --------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
        "| Math Foundations | Linear Algebra, Probability, Statistics, Calculus, Optimization | *Deep Learning Book* (Goodfellow), MIT OCW, Khan Academy          |\n",
        "| ML Algorithms    | Regression, Trees, SVM, Clustering, Ensembles                   | *Pattern Recognition and Machine Learning* (Bishop), sklearn docs |\n",
        "| Deep Learning    | NNs, CNNs, RNNs, Transformers, RL                               | *Dive into Deep Learning*, PyTorch & TensorFlow docs              |\n",
        "| Projects         | Tabular, NLP, CV, RL                                            | Kaggle datasets, HuggingFace Datasets                             |\n",
        "| Deployment       | Serving, APIs, MLOps                                            | TensorFlow Serving, MLflow, FastAPI                               |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 Step-by-Step Learning Cycle (Every Topic)\n",
        "\n",
        "1. **Theory/Intuition** – Understand why the algorithm exists.\n",
        "2. **Math** – Derive key equations step by step.\n",
        "3. **Code** – Implement in:\n",
        "\n",
        "   * scikit-learn\n",
        "   * TensorFlow\n",
        "   * Keras\n",
        "   * PyTorch\n",
        "4. **Experiments** – Tune hyperparameters, compare performance.\n",
        "5. **Analysis** – Report metrics, plots, and interpretations.\n",
        "6. **Project** – Apply algorithm to real dataset.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "H4fEZyQ4Bvh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Week 1 – Day 1: Vectors in Machine Learning\n",
        "---\n",
        "\n",
        "### 1. What is a Vector?\n",
        "\n",
        "* **Intuition**: A vector is just an **ordered list of numbers** that represents something with both **magnitude and direction**.\n",
        "* **Formal definition**:\n",
        "  A vector in $\\mathbb{R}^n$ is an element of the form:\n",
        "\n",
        "  $$\n",
        "  \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "  where each $v_i$ is a real number.\n",
        "\n",
        "📍 Example:\n",
        "\n",
        "* In $\\mathbb{R}^2$: $\\mathbf{v} = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$\n",
        "* In $\\mathbb{R}^3$: $\\mathbf{u} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 5 \\end{bmatrix}$\n",
        "\n",
        "👉 **ML Connection**: In machine learning, a **data point** with features is often stored as a vector.\n",
        "\n",
        "* Example: An image pixel row = vector.\n",
        "* A dataset row = vector of features.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Vector Operations\n",
        "\n",
        "#### (a) **Addition**\n",
        "\n",
        "$$\n",
        "\\mathbf{a} + \\mathbf{b} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} +\n",
        "\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n \\end{bmatrix} =\n",
        "\\begin{bmatrix} a_1 + b_1 \\\\ a_2 + b_2 \\\\ \\vdots \\\\ a_n + b_n \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Example:\n",
        "$\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 2 \\end{bmatrix}$\n",
        "\n",
        "---\n",
        "\n",
        "#### (b) **Scalar Multiplication**\n",
        "\n",
        "$$\n",
        "c \\cdot \\mathbf{v} = c \\cdot \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} =\n",
        "\\begin{bmatrix} c v_1 \\\\ c v_2 \\\\ \\vdots \\\\ c v_n \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Example:\n",
        "$2 \\cdot \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix}$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Coding with NumPy (Python)"
      ],
      "metadata": {
        "id": "oTc--naDB8bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# define vectors\n",
        "a = np.array([3, 4])\n",
        "b = np.array([1, -2])\n",
        "\n",
        "# addition\n",
        "print(\"a + b =\", a + b)\n",
        "\n",
        "# scalar multiplication\n",
        "print(\"2 * a =\", 2 * a)"
      ],
      "metadata": {
        "id": "RMOFraYjB4LV",
        "outputId": "f746adf6-1734-4163-bc72-6023a87de447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a + b = [4 2]\n",
            "2 * a = [6 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "BWnShSzSCMkS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElyVRwXMCHiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}